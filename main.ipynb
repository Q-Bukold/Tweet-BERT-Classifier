{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sources:\n",
    "https://towardsdatascience.com/tensorflow-and-transformers-df6fceaf57cc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.1\n",
      "4.31.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at dbmdz/bert-base-german-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at dbmdz/bert-base-german-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # or any {'0', '1', '2'}\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "print(tf.__version__)\n",
    "print(transformers.__version__)\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-uncased\")\n",
    "bert = TFAutoModel.from_pretrained(\"dbmdz/bert-base-german-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " attention_mask (InputLayer)    [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model_1 (TFBertModel)  TFBaseModelOutputWi  109927680   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_mask[0][0]']         \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 50,                                                \n",
      "                                768),                                                             \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 768)         0           ['tf_bert_model_1[0][0]']        \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 768)         3072        ['global_max_pooling1d[0][0]']   \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 128)          98432       ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " dropout_74 (Dropout)           (None, 128)          0           ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      " outputs (Dense)                (None, 2)            258         ['dropout_74[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 110,029,442\n",
      "Trainable params: 100,226\n",
      "Non-trainable params: 109,929,216\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input layers (has to be same structure as dataset)\n",
    "SEQ_LEN = 50 \n",
    "input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), name='input_ids', dtype='int32')\n",
    "mask = tf.keras.layers.Input(shape=(SEQ_LEN,), name='attention_mask', dtype='int32')\n",
    "\n",
    "# add layers\n",
    "embeddings = bert(input_ids, attention_mask=mask)[0]  # we only keep tensor 0 (last_hidden_state) of BERT\n",
    "X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\n",
    "X = tf.keras.layers.BatchNormalization()(X)\n",
    "X = tf.keras.layers.Dense(128, activation='relu')(X)\n",
    "X = tf.keras.layers.Dropout(0.1)(X)\n",
    "layers = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)  # adjust based on number of classes\n",
    "\n",
    "# Create model instance\n",
    "model = tf.keras.Model(inputs=[input_ids, mask], outputs=layers)\n",
    "\n",
    "#freeze BERT model\n",
    "model.layers[2].trainable = False #BERT is already well trained and has a lot of Parameters\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import metrics\n",
    "\n",
    "# compile model\n",
    "optimizer = tf.keras.optimizers.Adam(0.01)\n",
    "loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc = tf.keras.metrics.CategoricalAccuracy('accuracy')\n",
    "auprc = tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve='PR',\n",
    "    summation_method='interpolation',\n",
    "    name=None,\n",
    "    dtype=None,\n",
    "    thresholds=None,\n",
    "    multi_label=False,\n",
    "    num_labels=None,\n",
    "    label_weights=None,\n",
    "    from_logits=False\n",
    ")\n",
    "roc = tf.keras.metrics.AUC(\n",
    "    num_thresholds=200,\n",
    "    curve='ROC',\n",
    "    summation_method='interpolation',\n",
    "    name=None,\n",
    "    dtype=None,\n",
    "    thresholds=None,\n",
    "    multi_label=False,\n",
    "    num_labels=None,\n",
    "    label_weights=None,\n",
    "    from_logits=False\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[acc, auprc, roc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize AnnotationData and Create Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "0    1700\n",
      "1     673\n",
      "Name: count, dtype: int64\n",
      "RT @NDRinfo: Die deutsche Klimaaktivistin Luisa Neubauer wirft Kanzlerin Merkel wegen ihrer fehlenden Unterstützung für den europäischen Kl…\n",
      "[  102.  1939. 13523.  7774.  4855. 25358. 30949. 20789.  1939.  1648.\n",
      " 25358. 30949.   552.  1939.  1054.  1493. 25358.   296. 25358. 30949.\n",
      "  2719.   142.   468.   524.   552.  6222.   847.  1061.  1061.   160.\n",
      "   552.   928.  1061.  1138.  7774. 30948.  2828. 30972. 30964. 17800.\n",
      "   103.     0.     0.     0.     0.     0.     0.     0.     0.     0.]\n",
      "length of dataset = 2373\n",
      "<class 'tensorflow.python.data.ops.map_op._MapDataset'>\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer\n",
    "\n",
    "# own modules:\n",
    "from bertconfig import tokenize, encode_values\n",
    "from my_utils import load_hasoc\n",
    "\n",
    "# Preprocess\n",
    "## load and encode label\n",
    "df = load_hasoc(\"data/hasoc_2020_de_train_new_a.xlsx\")\n",
    "## drop duplicates\n",
    "df.drop_duplicates(subset=\"text\", keep=\"first\", inplace=True)\n",
    "print(df.value_counts(\"label\"))\n",
    "\n",
    "# encode values\n",
    "arr = df['label'].values  # label column in df -> array\n",
    "labels = encode_values(arr) #-> makes [0,1] or [1,0] from 0 or 1\n",
    "\n",
    "# tokenize comments\n",
    "# set max token length of comment\n",
    "SEQ_LEN = 50\n",
    "\n",
    "# initialize two arrays for input tensors and loop through data and tokenize everything\n",
    "all_ids = np.zeros((len(df), SEQ_LEN))\n",
    "all_mask = np.zeros((len(df), SEQ_LEN))\n",
    "for i, sentence in enumerate(df['text']):\n",
    "    tokens = tokenize(sentence, tokenizer, SEQ_LEN)\n",
    "    # append ID of every token in sentence:\n",
    "    # append Mask (1 if valid word, 0 if padding)\n",
    "    all_ids[i, :] = tokens['input_ids']\n",
    "    all_mask[i, :] = tokens['attention_mask']\n",
    "\n",
    "print(df['text'].iloc[1])\n",
    "print(all_ids[0])\n",
    "\n",
    "# create tensorflow dataset object\n",
    "dataset = tf.data.Dataset.from_tensor_slices((all_ids, all_mask, labels))\n",
    "\n",
    "# restructure dataset format for BERT\n",
    "def map_func(input_ids, masks, labels):\n",
    "    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n",
    "\n",
    "dataset = dataset.map(map_func)  # apply the mapping function % CREATE DATASET\n",
    "print(\"length of dataset = {}\".format(dataset.cardinality().numpy()))\n",
    "print(type(dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of Batches dataset = 75\n",
      "number of Batches train_dataset = 52\n",
      "number of Batches val_dataset = 12\n",
      "number of Batches test_dataset = 11\n"
     ]
    }
   ],
   "source": [
    "# shuffle and batch the dataset\n",
    "dataset_batched = dataset.shuffle(10000).batch(32) ## created _BatchDataset\n",
    "DS_LEN = dataset_batched.cardinality().numpy()  # get dataset length\n",
    "print(\"number of Batches dataset = {}\".format(DS_LEN))\n",
    "\n",
    "train_size = round(0.7 * DS_LEN)\n",
    "val_size = round(0.15 * DS_LEN)\n",
    "test_size = round(0.15 * DS_LEN)\n",
    "test_dataset = dataset_batched.skip(train_size)\n",
    "\n",
    "train_dataset = dataset_batched.take(train_size)\n",
    "val_dataset = test_dataset.skip(val_size)\n",
    "test_dataset = test_dataset.take(test_size)\n",
    "\n",
    "print(\"number of Batches train_dataset = {}\".format(train_dataset.cardinality().numpy()))\n",
    "print(\"number of Batches val_dataset = {}\".format(val_dataset.cardinality().numpy()))\n",
    "print(\"number of Batches test_dataset = {}\".format(test_dataset.cardinality().numpy()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n",
      "75/75 [==============================] - 122s 1s/step - loss: 0.8456 - accuracy: 0.7324 - auc: 0.7644 - auc_1: 0.7875 - val_loss: 0.5690 - val_accuracy: 0.7507 - val_auc: 0.8348 - val_auc_1: 0.8376\n"
     ]
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "from tensorflow import keras\n",
    "import tensorboard\n",
    "\n",
    "# Define the Keras TensorBoard callback.\n",
    "logdir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "\n",
    "# train model (metrics: acc, auprc, roc)\n",
    "history = model.fit(dataset_batched,\n",
    "                    epochs=1,\n",
    "                    validation_data=val_dataset,\n",
    "                    callbacks=[tensorboard_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43075522780418396]\n"
     ]
    }
   ],
   "source": [
    "print(model.history.history['val_auc_3'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!kill 1363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1363), started 0:02:47 ago. (Use '!kill 1363' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-174259aa7bfb9c7b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-174259aa7bfb9c7b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
