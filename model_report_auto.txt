'''
    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), name='input_ids', dtype='int32')
    mask = tf.keras.layers.Input(shape=(SEQ_LEN,), name='attention_mask', dtype='int32')

    # add layers
    embeddings = bert(input_ids, attention_mask=mask)[0]  # we only keep tensor 0 (last_hidden_state) of BERT
    X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality
    X = tf.keras.layers.BatchNormalization()(X)
    X = tf.keras.layers.Dense(128, activation='relu')(X)
    X = tf.keras.layers.Dropout(0.1)(X)
    layers = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)  # adjust based on number of classes
'''

Sun00:02:40
model:
  SEQ_LEN: 50
  batch_size: 64
  epochs: 5
  learning_rate: 0.01
[0.4525306224822998, 0.8423529267311096, 0.5478435754776001]
SunXXXXXX

Sun01:05:54
SEQ_LEN: 50
batch_size: 32
epochs: 10
learning_rate: 0.01
loss	accuracy	f1_score
0.5225393772125244	0.8435294032096863	0.5166717171669006
Sun01:13:54

Sun01:13:58
SEQ_LEN: 50
batch_size: 64
epochs: 10
learning_rate: 0.01
loss	accuracy	f1_score
0.540678083896637	0.8388235569000244	0.4899054169654846
Sun01:21:26

Sun01:21:31
SEQ_LEN: 50
batch_size: 64
epochs: 10
learning_rate: 2.0e-05
loss	accuracy	f1_score
0.47134023904800415	0.822352945804596	0.5190632343292236
Sun01:29:10

'''
    input_ids = tf.keras.layers.Input(shape=(SEQ_LEN,), name='input_ids', dtype='int32')
    mask = tf.keras.layers.Input(shape=(SEQ_LEN,), name='attention_mask', dtype='int32')

    # add layers
    embeddings = bert(input_ids, attention_mask=mask)[0]  # we only keep tensor 0 (last_hidden_state) of BERT
    
    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(SEQ_LEN))(embeddings)
    X = tf.keras.layers.Dense(SEQ_LEN, activation='relu')(X)
    X = tf.keras.layers.Dropout(0.1)(X)
    layers = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)  # adjust based on number of classes
'''

Sun01:42:36
SEQ_LEN: 50
batch_size: 32
epochs: 10
learning_rate: 0.01
loss	accuracy	f1_score
0.42949727177619934	0.8176470398902893	0.584031879901886
509.263370513916
Sun01:50:57

Sun01:51:02
SEQ_LEN: 50
batch_size: 32
epochs: 10
learning_rate: 0.01
loss	accuracy	f1_score
0.48791077733039856	0.8399999737739563	0.45652174949645996
506.2640960216522
Sun01:59:24

Sun01:59:29
SEQ_LEN: 50
batch_size: 64
epochs: 20 <---
learning_rate: 0.01
loss	accuracy	f1_score
0.4402269721031189	0.8211764693260193	0.5454801321029663
914.9661977291107
Sun02:14:39


'''
REDUCED val_size to 0.1
!IDEA = Introduce more Dropout, to better generalize Modell for different Data
    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(SEQ_LEN))(embeddings)
    X = tf.keras.layers.Dropout(0.1)(X)
    X = tf.keras.layers.Dense(SEQ_LEN, activation='relu')(X)
    X = tf.keras.layers.Dropout(0.2)(X)
    layers = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)  # adjust based on number of classes
'''

Sun02:16:31
SEQ_LEN: 50
batch_size: 32
epochs: 10
learning_rate: 0.01
loss	accuracy	f1_score
0.4322429597377777	0.841176450252533	0.5565028786659241
525.7339441776276
Sun02:25:09

Sun02:25:14
SEQ_LEN: 50
batch_size: 32
epochs: 10
learning_rate: 0.01
loss	accuracy	f1_score
0.4285900592803955	0.8376470804214478	0.6020815372467041
524.2070446014404
Sun02:33:53

'''
REDUCED val_size to 0.1
!IDEA = Introduce more Dropout, to better generalize Modell for different Data
    X = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(SEQ_LEN))(embeddings)
    X = tf.keras.layers.Dropout(0.1)(X)
    X = tf.keras.layers.Dense(SEQ_LEN, activation='relu')(X)
    X = tf.keras.layers.Dropout(0.2)(X)
    layers = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(X)  # adjust based on number of classes
'''

Sun02:16:31
SEQ_LEN: 50
batch_size: 32
epochs: 10
learning_rate: 0.01
loss	accuracy	f1_score
0.4322429597377777	0.841176450252533	0.5565028786659241
525.7339441776276
Sun02:25:09

Sun02:25:14
SEQ_LEN: 50
batch_size: 32
epochs: 10
learning_rate: 0.01
loss	accuracy	f1_score
0.4285900592803955	0.8376470804214478	0.6020815372467041
524.2070446014404
Sun02:33:53



